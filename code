import cv2
import mediapipe as mp
import numpy as np
import time

# Initialize mediapipe face mesh detector
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False,
                                  max_num_faces=1,
                                  refine_landmarks=True,
                                  min_detection_confidence=0.5,
                                  min_tracking_confidence=0.5)

# Drawing utils
mp_drawing = mp.solutions.drawing_utils
drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

# EAR calculation
def eye_aspect_ratio(eye_points):
    A = np.linalg.norm(eye_points[1] - eye_points[5])  # vertical
    B = np.linalg.norm(eye_points[2] - eye_points[4])  # vertical
    C = np.linalg.norm(eye_points[0] - eye_points[3])  # horizontal
    ear = (A + B) / (2.0 * C)
    return ear

# MAR calculation
def mouth_aspect_ratio(mouth_landmarks):
    A = np.linalg.norm(mouth_landmarks[2] - mouth_landmarks[6])  # vertical
    C = np.linalg.norm(mouth_landmarks[0] - mouth_landmarks[4])  # horizontal
    mar = A / C
    return mar

# Landmark indices (based on MediaPipe)
left_eye_idx = [362, 385, 387, 263, 373, 380]
right_eye_idx = [33, 160, 158, 133, 153, 144]
mouth_idx = [78, 81, 13, 311, 308, 402, 14, 317]

# Thresholds
EAR_THRESHOLD = 0.25
EAR_CONSEC_FRAMES = 15
MAR_THRESHOLD = 0.7

# Counters
frame_counter = 0

# Start video capture
cap = cv2.VideoCapture(0)

while cap.isOpened():
    success, frame = cap.read()
    if not success:
        print("Ignoring empty frame.")
        continue

    h, w = frame.shape[:2]
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = face_mesh.process(rgb_frame)

    if results.multi_face_landmarks:
        for face_landmarks in results.multi_face_landmarks:
            # Extract eye and mouth landmarks
            landmarks = face_landmarks.landmark
            left_eye = np.array([(landmarks[i].x * w, landmarks[i].y * h) for i in left_eye_idx])
            right_eye = np.array([(landmarks[i].x * w, landmarks[i].y * h) for i in right_eye_idx])
            mouth = np.array([(landmarks[i].x * w, landmarks[i].y * h) for i in mouth_idx])

            # EAR calculation
            left_ear = eye_aspect_ratio(left_eye)
            right_ear = eye_aspect_ratio(right_eye)
            ear = (left_ear + right_ear) / 2.0

            # MAR calculation
            mar = mouth_aspect_ratio(mouth)

            # Draw status
            cv2.putText(frame, f'EAR: {ear:.2f}', (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            cv2.putText(frame, f'MAR: {mar:.2f}', (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

            # Check for drowsiness
            if ear < EAR_THRESHOLD:
                frame_counter += 1
                if frame_counter >= EAR_CONSEC_FRAMES:
                    cv2.putText(frame, "DROWSINESS ALERT!", (150, 200), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)
            else:
                frame_counter = 0

            # Check for yawning
            if mar > MAR_THRESHOLD:
                cv2.putText(frame, "YAWNING DETECTED!", (150, 240), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 255), 3)

            # Optional: Draw landmarks
            mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_CONTOURS,
                                      landmark_drawing_spec=drawing_spec,
                                      connection_drawing_spec=drawing_spec)

    cv2.imshow("Drowsiness Detection", frame)
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()

